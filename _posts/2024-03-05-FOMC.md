# Aspect Based Sentiment Analysis_FOMC minutes

## Introduction

This project aims to utilize a large language model(LLM) to extract the official attitudes of the U.S. central bank towards domestic inflation and financial markets from its documents. Subsequently, appropriate Markov state transition models are applied to elucidate the evolving relationship between the two over time. The results indicate that from 2006 to 2023, the relationship between them is...

- **Input:**

  8 FOMC minutes documents per year, totaling 143 documents over 17 years, excerpt content as shown in Figure 1.

   [FOMC minutes](https://www.federalreserve.gov/monetarypolicy/fomccalendars.htm)
   ![](/images/FOMCminutes.png "Figure 1")


- **Middle Output:**

   Employing the Latent Dirichlet Allocation (LDA) model for topic identification within the texts, three representative topics are identified: Economic Outlook, Financial Market, and Inflation. An interactive tool below presents the most frequently occurring words for each topic.



- **Final Output:**

  Utilizing the financial-specific language model FinBERT to interpret the sentiment of each topic within the texts. This yields sentiment scores regarding the official attitudes towards each topic from 2006 to 2023. Figure 2 below illustrates the sentiment time series for the year 2021, further refined by a Hidden Markov Model to discern the official attitudes towards Financial Market and Inflation.


   ![](/images/Sentiment.png "Figure 2")

## Analysis Step

1. 

   1.1. 精益求精 - Dealing with Background Noise in Images
   
   1.2. 斬草不除根 - How to Remove Background Noise from Images
   
   1.3. 書同文?車同軌? - Conversion to Vectors and Normalization issues
   
2. Model Exploration
   
   2.1. 長江後浪推前浪 - Model Comparison
   
   2.2. 站在巨人肩膀上 - Model Pretraining
   
   2.3. 工欲善其事 - Hyperparameter Selection and Visualization
   
   2.4. 心中一把尺 - Model Evaluation


## Technical Challenge

### Data Preprocessing
- Data augmentation techniques

  Implementing data augmentation techniques may help to enhance out of sample prediction accuracy. But how those augmented data should be incorporated into the data pipeline? Using local or streaming method?

### Model Building

#### UNet

![](/images/UNET.png "UNet structure")

- **Feature extractor**: In segmentation models, feature extractor layers are used throughout the network, allowing it to process spatial information efficiently. These layers capture local patterns and structures in the input data.

- **Skip Connections**: To recover fine-grained details lost during downsampling, Unet uses skip connections. These connections combine feature maps from early layers with those from later layers, aiding in the reconstruction of high-resolution information.

- **Upsampling**: Segmentation models employ upsampling techniques to restore the spatial resolution of the feature maps. Transposed convolutions or bilinear interpolation can be used for this purpose.

#### Challenge encountered

- Last or Best parameter set

  After training each epoch, tens of thousands of estimated model parameters will be logged and the best set among all epochs will be chosen in the condition that smallest validation loss. However, parameter set from last epoch is not necessary to be the best set. So, how to determine whether training with more epochs further or taking the best set in the current training process is the trade-off between prediction accuracy and the time consumption.

- Countless combinations of hyperparameters

  There are infinite combinations of hyperparameters, in my project I mainly focus on trying different combinations of learning rate and weight decay using grid search method. With the help of visualization provided by wandb, it is easier to compare and understand the performance of each combination comprehensively and quickly.
  
- Validation loss remain the same

  I encounter the problem of the not declined validation loss. The predictions are identical, increasing the epochs may not improve the model's performance. Trying the different learning rate using scheduler function help to avoid getting stuck in a local minimum of the loss function. 

- Shape of input tensor 

  I encounter the problem of the not declined validation loss. The predictions are identical, increasing the epochs may not improve the model's performance. Trying the different learning rate using scheduler function help to avoid getting stuck in a local minimum of the loss function.
  
### Evaluation


## Introduction


## Challenge



## Data Preparation
I define a class `FedMinScraper` aimed at extracting monthly US Federal Reserve minutes from the Federal Reserve website. It takes a list of dates as input, specifying the periods for which transcripts are to be extracted. The class utilizes multithreading for faster extraction and parsing of the transcripts. 

```python
class FedMinScraper(object):
    """
    The purpose of this class is to extract monthly US federal reserve minutes
    
    Parameters
    ----------
    dates: list('yyyy'|'yyyy-mm')
        List of strings/integers referencing dates for extraction
        Example:
        dates = [min_year] -> Extracts all transcripts for this year
        dates = [min_year,max_year] -> Extracts transactions for a set of years
        dates = ['2020-01'] -> Extracts transcripts for a single month/year

    nthreads: int
        Set of threads used for multiprocessing
        defaults to None

    Returns
    --------
    transcripts: txt files

    """

    url_parent = r"https://www.federalreserve.gov/monetarypolicy/"
    url_current = r"fomccalendars.htm"

    # historical transcripts are stored differently
    url_historical = r"fomchistorical{}.htm"
    # each transcript has a unique address, gathered from url_current or url_historical
    url_transcript = r"fomcminutes{}.htm"
    href_regex = re.compile("(?i)/fomc[/]?minutes[/]?\d{8}.htm")

    def __init__(self, dates, nthreads=5, save_path=None):

        # make sure user has given list with strings
        if not isinstance(dates, list):
            raise TypeError("dates should be a list of yyyy or yyyymm str/int")

        elif not all([bool(re.search(r"^\d{4}$|^\d{6}$", str(d))) for d in dates]):
            raise ValueError("dates should be in a yyyy or yyyymm format")

        self.dates = dates
        self.nthreads = nthreads
        self.save_path = save_path

        self.ndates = len(dates)
        self.years = [int(d[:4]) for d in dates]
        self.min_year, self.max_year = min(self.years), max(self.years)
        self.transcript_dates = []
        self.transcripts = {}
        self.historical_date = None

        self._get_transcript_dates()

        self.start_threading()

        if save_path:
            self.save_transcript()

    def _get_transcript_dates(self):
        """
        Extract all links for
        """

        r = requests.get(urljoin(FedMinScraper.url_parent, FedMinScraper.url_current))
        soup = BeautifulSoup(r.text, "lxml")
        # dates are given by yyyymmdd

        tdates = soup.findAll("a", href=self.href_regex)
        tdates = [re.search(r"\d{8}", str(t))[0] for t in tdates]
        self.historical_date = int(min(tdates)[:4])
        # find minimum year

        # extract all of these and filter
        # tdates can only be applied to /fomcminutes
        # historical dates need to be applied to federalreserve.gov

        if self.min_year < self.historical_date:
            # just append the years i'm interested in
            for y in range(self.min_year, self.historical_date + 1):

                r = requests.get(
                    urljoin(
                        FedMinScraper.url_parent, FedMinScraper.url_historical.format(y)
                    )
                )
                soup = BeautifulSoup(r.text, parser="lxml")
                hdates = soup.find_all("a", href=self.href_regex)
                tdates.extend([re.search(r"\d{8}", str(t))[0] for t in hdates])

        self.transcript_dates = tdates

    def get_transcript(self, transcript_date):

        transcript_url = urljoin(
            FedMinScraper.url_parent,
            FedMinScraper.url_transcript.format(transcript_date),
        )
        r = requests.get(transcript_url)

        if not r.ok:
            transcript_url = urljoin(
                FedMinScraper.url_parent.replace("/monetarypolicy", ""),
                r"fomc/minutes/{}.htm".format(transcript_date),
            )
            r = requests.get(transcript_url)

        soup = BeautifulSoup(r.content, "lxml")
        main_text = soup.findAll(name="p")

        clean_main_text = "\n\n".join(t.text.strip() for t in main_text)

        # reduce double spaces to one
        clean_main_text = re.sub(r"  ", r" ", clean_main_text)

        self.transcripts[transcript_date] = clean_main_text

    def start_threading(self):
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.nthreads) as executor:
            executor.map(self.get_transcript, self.transcript_dates)

    def save_transcript(self):
        if not os.path.exists(self.save_path):
            os.makedirs(self.save_path)

        for fname, txt in self.transcripts.items():
            with open(
                os.path.join(self.save_path, fname + ".txt"), "w", encoding="utf-8"
            ) as o:
                o.write(txt)
                o.close()
```

The class uses BeautifulSoup for web scraping, fetching the URLs of the transcripts based on the provided dates. It then extracts the main text content from the fetched transcripts, cleans the text, and stores the cleaned transcripts in a dictionary with keys representing the respective dates. 

The extracted transcripts can also be saved to a specified directory if provided. The class verifies the input dates and ensures they are in the correct format before proceeding with extraction. It distinguishes between current and historical transcripts based on the provided dates and constructs the appropriate URLs for extraction.

Overall, this class provides a convenient way to automate the retrieval and storage of Federal Reserve meeting transcripts for analysis or archival purposes.

After having the FOMC minutes text documents, I truncated the documents discarding the unnecessary information such as the list of attendants presented in the begining of the FOMC minute. 
![](/images/minute.png "Attendance info should be truncated before NLP")

## EDA

### Descriptive Statistics

![](/images/EDA.png "minute information")

### Paragraphs and words overtime

![](/images/year.png "paragraph and word over time")

## Model Building


## Metrics



