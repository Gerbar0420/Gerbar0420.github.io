# Natural language processing_FOMC Minutes

## Introduction


## Challenge



## Data Preparation
I define a class `FedMinScraper` aimed at extracting monthly US Federal Reserve minutes from the Federal Reserve website. It takes a list of dates as input, specifying the periods for which transcripts are to be extracted. The class utilizes multithreading for faster extraction and parsing of the transcripts. 

```python
class FedMinScraper(object):
    """
    The purpose of this class is to extract monthly US federal reserve minutes
    
    Parameters
    ----------
    dates: list('yyyy'|'yyyy-mm')
        List of strings/integers referencing dates for extraction
        Example:
        dates = [min_year] -> Extracts all transcripts for this year
        dates = [min_year,max_year] -> Extracts transactions for a set of years
        dates = ['2020-01'] -> Extracts transcripts for a single month/year

    nthreads: int
        Set of threads used for multiprocessing
        defaults to None

    Returns
    --------
    transcripts: txt files

    """

    url_parent = r"https://www.federalreserve.gov/monetarypolicy/"
    url_current = r"fomccalendars.htm"

    # historical transcripts are stored differently
    url_historical = r"fomchistorical{}.htm"
    # each transcript has a unique address, gathered from url_current or url_historical
    url_transcript = r"fomcminutes{}.htm"
    href_regex = re.compile("(?i)/fomc[/]?minutes[/]?\d{8}.htm")

    def __init__(self, dates, nthreads=5, save_path=None):

        # make sure user has given list with strings
        if not isinstance(dates, list):
            raise TypeError("dates should be a list of yyyy or yyyymm str/int")

        elif not all([bool(re.search(r"^\d{4}$|^\d{6}$", str(d))) for d in dates]):
            raise ValueError("dates should be in a yyyy or yyyymm format")

        self.dates = dates
        self.nthreads = nthreads
        self.save_path = save_path

        self.ndates = len(dates)
        self.years = [int(d[:4]) for d in dates]
        self.min_year, self.max_year = min(self.years), max(self.years)
        self.transcript_dates = []
        self.transcripts = {}
        self.historical_date = None

        self._get_transcript_dates()

        self.start_threading()

        if save_path:
            self.save_transcript()

    def _get_transcript_dates(self):
        """
        Extract all links for
        """

        r = requests.get(urljoin(FedMinScraper.url_parent, FedMinScraper.url_current))
        soup = BeautifulSoup(r.text, "lxml")
        # dates are given by yyyymmdd

        tdates = soup.findAll("a", href=self.href_regex)
        tdates = [re.search(r"\d{8}", str(t))[0] for t in tdates]
        self.historical_date = int(min(tdates)[:4])
        # find minimum year

        # extract all of these and filter
        # tdates can only be applied to /fomcminutes
        # historical dates need to be applied to federalreserve.gov

        if self.min_year < self.historical_date:
            # just append the years i'm interested in
            for y in range(self.min_year, self.historical_date + 1):

                r = requests.get(
                    urljoin(
                        FedMinScraper.url_parent, FedMinScraper.url_historical.format(y)
                    )
                )
                soup = BeautifulSoup(r.text, parser="lxml")
                hdates = soup.find_all("a", href=self.href_regex)
                tdates.extend([re.search(r"\d{8}", str(t))[0] for t in hdates])

        self.transcript_dates = tdates

    def get_transcript(self, transcript_date):

        transcript_url = urljoin(
            FedMinScraper.url_parent,
            FedMinScraper.url_transcript.format(transcript_date),
        )
        r = requests.get(transcript_url)

        if not r.ok:
            transcript_url = urljoin(
                FedMinScraper.url_parent.replace("/monetarypolicy", ""),
                r"fomc/minutes/{}.htm".format(transcript_date),
            )
            r = requests.get(transcript_url)

        soup = BeautifulSoup(r.content, "lxml")
        main_text = soup.findAll(name="p")

        clean_main_text = "\n\n".join(t.text.strip() for t in main_text)

        # reduce double spaces to one
        clean_main_text = re.sub(r"  ", r" ", clean_main_text)

        self.transcripts[transcript_date] = clean_main_text

    def start_threading(self):
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.nthreads) as executor:
            executor.map(self.get_transcript, self.transcript_dates)

    def save_transcript(self):
        if not os.path.exists(self.save_path):
            os.makedirs(self.save_path)

        for fname, txt in self.transcripts.items():
            with open(
                os.path.join(self.save_path, fname + ".txt"), "w", encoding="utf-8"
            ) as o:
                o.write(txt)
                o.close()
```

The class uses BeautifulSoup for web scraping, fetching the URLs of the transcripts based on the provided dates. It then extracts the main text content from the fetched transcripts, cleans the text, and stores the cleaned transcripts in a dictionary with keys representing the respective dates. 

The extracted transcripts can also be saved to a specified directory if provided. The class verifies the input dates and ensures they are in the correct format before proceeding with extraction. It distinguishes between current and historical transcripts based on the provided dates and constructs the appropriate URLs for extraction.

Overall, this class provides a convenient way to automate the retrieval and storage of Federal Reserve meeting transcripts for analysis or archival purposes.

After having the FOMC minutes text documents, I truncated the documents discarding the unnecessary information such as the list of attendants presented in the begining of the FOMC minute. 
![](/images/minute.png "Attendance info should be truncated before NLP")

## EDA

### Descriptive Statistics

![](/images/EDA.png "minute information")

### Paragraphs and words overtime

![](/images/year.png "paragraph and word over time")

## Model Building


## Metrics



